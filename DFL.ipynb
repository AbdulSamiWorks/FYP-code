{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fedml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e21c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed659717",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flsim.interfaces.model_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflsim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterfaces\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelManager\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflsim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprivacy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrivacySetting\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflsim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictConfigWrapper\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flsim.interfaces.model_manager'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import ast\n",
    "from flsim.interfaces.model_manager import ModelManager\n",
    "from flsim.privacy.common import PrivacySetting\n",
    "from flsim.utils.config_utils import DictConfigWrapper\n",
    "from flsim.trainers.sync_trainer import SyncTrainer\n",
    "from flsim.data.data_loader import FLDataLoader\n",
    "from flsim.utils.simple_model import SimpleConvNet  # Just a dummy fallback\n",
    "from flsim.utils.example_models import ModelTrainer\n",
    "from flsim.channels.message import Message\n",
    "from flsim.optimizers.async_optimizers import FedAvgWithLRDecay\n",
    "import hydra\n",
    "import logging\n",
    "\n",
    "# ==== Setup ====\n",
    "base_path = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/ODIR-5K/ODIR-5K\"\n",
    "train_images_dir = os.path.join(base_path, \"Training Images\")\n",
    "data_file = os.path.join(base_path, \"data.xlsx\")\n",
    "csv_dir = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/first_raw/\"\n",
    "\n",
    "# ==== Preprocess ====\n",
    "df = pd.read_excel(data_file)\n",
    "label_cols = [\"N\",\"D\",\"G\",\"C\",\"A\",\"H\",\"M\",\"O\"]\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    rows.append({\"filename\": r[\"Left-Fundus\"], \"labels\": r[label_cols].astype(int).tolist()})\n",
    "    rows.append({\"filename\": r[\"Right-Fundus\"], \"labels\": r[label_cols].astype(int).tolist()})\n",
    "df_images = pd.DataFrame(rows)\n",
    "df_images[\"labels\"] = df_images[\"labels\"].apply(ast.literal_eval)\n",
    "\n",
    "# ==== Dataset ====\n",
    "class OcularDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row[\"filename\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "        return self.transform(image), label\n",
    "\n",
    "# ==== Split clients ====\n",
    "NUM_CLIENTS = 5\n",
    "splits = random_split(df_images, [len(df_images)//NUM_CLIENTS]*NUM_CLIENTS)\n",
    "clients_data = [OcularDataset(s.dataset.iloc[s.indices], train_images_dir) for s in splits]\n",
    "\n",
    "# ==== Model ====\n",
    "def get_model():\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "    model._fc = nn.Linear(model._fc.in_features, 8)\n",
    "    return model\n",
    "\n",
    "# ==== Trainer ====\n",
    "class CustomTrainer(ModelTrainer):\n",
    "    def __init__(self, model, optimizer, criterion, device, client_id):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        self.model.train()\n",
    "        for epoch in range(3):\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(x)\n",
    "                loss = self.criterion(output, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, val_loader, round_num):\n",
    "        self.model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                output = self.model(x)\n",
    "                loss = self.criterion(output, y)\n",
    "                total_loss += loss.item()\n",
    "                preds = (torch.sigmoid(output) > 0.3).int().cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend(preds)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "        # Save CSV\n",
    "        csv_path = os.path.join(csv_dir, f\"DFL_Client_{self.client_id}.csv\")\n",
    "        log = pd.DataFrame([[round_num, total_loss, acc, prec, rec, f1]],\n",
    "                           columns=[\"round\", \"loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "        if os.path.exists(csv_path):\n",
    "            log.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "        else:\n",
    "            log.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"Client {self.client_id} - Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "        return acc\n",
    "\n",
    "# ==== Training Loop ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.0]*8).to(device))\n",
    "NUM_ROUNDS = 20\n",
    "\n",
    "# Simulate Mesh DFL\n",
    "logs = [pd.DataFrame(columns=[\"round\", \"loss\", \"accuracy\", \"precision\", \"recall\", \"f1\"]) for _ in range(NUM_CLIENTS)]\n",
    "clients = []\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    model = get_model()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    ds = clients_data[i]\n",
    "    train_len = int(0.8 * len(ds))\n",
    "    train_ds, val_ds = random_split(ds, [train_len, len(ds)-train_len])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=8)\n",
    "    trainer = CustomTrainer(model, optimizer, criterion, device, i+1)\n",
    "    clients.append({\"trainer\": trainer, \"val_loader\": val_loader, \"train_loader\": train_loader})\n",
    "\n",
    "# Mesh update simulation: each client sends params to next neighbor\n",
    "for round_num in range(1, NUM_ROUNDS + 1):\n",
    "    print(f\"\\n=== Round {round_num} ===\")\n",
    "    # Train\n",
    "    for c in clients:\n",
    "        c[\"trainer\"].train(c[\"train_loader\"])\n",
    "    # Mesh-style param sharing\n",
    "    new_weights = [c[\"trainer\"].model.state_dict() for c in clients]\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        next_i = (i + 1) % NUM_CLIENTS\n",
    "        clients[next_i][\"trainer\"].model.load_state_dict(new_weights[i])\n",
    "    # Eval\n",
    "    for c in clients:\n",
    "        c[\"trainer\"].evaluate(c[\"val_loader\"], round_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e6aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277af5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cd948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4561f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to C:\\Users\\aitoo/.cache\\torch\\hub\\checkpoints\\efficientnet-b2-8bb594d6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35.1M/35.1M [01:44<00:00, 351kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "\n",
      "=== Round 1 ===\n",
      "Client 1 - Acc: 0.2929 | Prec: 0.4961 | Rec: 0.6339 | F1: 0.5342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\939386771.py:147: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 - Acc: 0.2964 | Prec: 0.5190 | Rec: 0.6512 | F1: 0.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\939386771.py:147: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 3 - Acc: 0.2393 | Prec: 0.4923 | Rec: 0.6399 | F1: 0.5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\939386771.py:147: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 4 - Acc: 0.2357 | Prec: 0.4440 | Rec: 0.5952 | F1: 0.4860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\939386771.py:147: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 5 - Acc: 0.2643 | Prec: 0.4595 | Rec: 0.6065 | F1: 0.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\939386771.py:147: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 2 ===\n",
      "Client 1 - Acc: 0.3036 | Prec: 0.5119 | Rec: 0.6173 | F1: 0.5371\n",
      "Client 2 - Acc: 0.3179 | Prec: 0.5155 | Rec: 0.6101 | F1: 0.5369\n",
      "Client 3 - Acc: 0.2857 | Prec: 0.5327 | Rec: 0.6601 | F1: 0.5614\n",
      "Client 4 - Acc: 0.2536 | Prec: 0.4628 | Rec: 0.6095 | F1: 0.5069\n",
      "Client 5 - Acc: 0.3500 | Prec: 0.5095 | Rec: 0.5851 | F1: 0.5269\n",
      "\n",
      "=== Round 3 ===\n",
      "Client 1 - Acc: 0.3786 | Prec: 0.5530 | Rec: 0.6494 | F1: 0.5780\n",
      "Client 2 - Acc: 0.3679 | Prec: 0.5446 | Rec: 0.6238 | F1: 0.5619\n",
      "Client 3 - Acc: 0.2964 | Prec: 0.4917 | Rec: 0.5690 | F1: 0.5060\n",
      "Client 4 - Acc: 0.2893 | Prec: 0.4643 | Rec: 0.5518 | F1: 0.4842\n",
      "Client 5 - Acc: 0.3286 | Prec: 0.5250 | Rec: 0.6387 | F1: 0.5545\n",
      "\n",
      "=== Round 4 ===\n",
      "Client 1 - Acc: 0.3714 | Prec: 0.5381 | Rec: 0.5917 | F1: 0.5439\n",
      "Client 2 - Acc: 0.3429 | Prec: 0.5268 | Rec: 0.6095 | F1: 0.5438\n",
      "Client 3 - Acc: 0.3393 | Prec: 0.5494 | Rec: 0.6274 | F1: 0.5614\n",
      "Client 4 - Acc: 0.2607 | Prec: 0.4685 | Rec: 0.5756 | F1: 0.4954\n",
      "Client 5 - Acc: 0.3500 | Prec: 0.5435 | Rec: 0.6369 | F1: 0.5639\n",
      "\n",
      "=== Round 5 ===\n",
      "Client 1 - Acc: 0.3357 | Prec: 0.5113 | Rec: 0.5720 | F1: 0.5183\n",
      "Client 2 - Acc: 0.3786 | Prec: 0.5464 | Rec: 0.6000 | F1: 0.5530\n",
      "Client 3 - Acc: 0.3321 | Prec: 0.5280 | Rec: 0.5899 | F1: 0.5354\n",
      "Client 4 - Acc: 0.2893 | Prec: 0.4613 | Rec: 0.5310 | F1: 0.4749\n",
      "Client 5 - Acc: 0.3250 | Prec: 0.5190 | Rec: 0.6238 | F1: 0.5463\n",
      "\n",
      "=== Round 6 ===\n",
      "Client 1 - Acc: 0.3643 | Prec: 0.5554 | Rec: 0.6095 | F1: 0.5588\n",
      "Client 2 - Acc: 0.3643 | Prec: 0.5310 | Rec: 0.5690 | F1: 0.5293\n",
      "Client 3 - Acc: 0.3250 | Prec: 0.5089 | Rec: 0.5685 | F1: 0.5170\n",
      "Client 4 - Acc: 0.2464 | Prec: 0.4744 | Rec: 0.5667 | F1: 0.4902\n",
      "Client 5 - Acc: 0.3679 | Prec: 0.5411 | Rec: 0.6333 | F1: 0.5639\n",
      "\n",
      "=== Round 7 ===\n",
      "Client 1 - Acc: 0.3571 | Prec: 0.5048 | Rec: 0.5500 | F1: 0.5085\n",
      "Client 2 - Acc: 0.3429 | Prec: 0.5119 | Rec: 0.5673 | F1: 0.5194\n",
      "Client 3 - Acc: 0.3679 | Prec: 0.5393 | Rec: 0.5732 | F1: 0.5371\n",
      "Client 4 - Acc: 0.3107 | Prec: 0.5054 | Rec: 0.5994 | F1: 0.5263\n",
      "Client 5 - Acc: 0.3714 | Prec: 0.5458 | Rec: 0.6119 | F1: 0.5585\n",
      "\n",
      "=== Round 8 ===\n",
      "Client 1 - Acc: 0.3536 | Prec: 0.5351 | Rec: 0.5851 | F1: 0.5385\n",
      "Client 2 - Acc: 0.3786 | Prec: 0.5196 | Rec: 0.5577 | F1: 0.5226\n",
      "Client 3 - Acc: 0.3607 | Prec: 0.5548 | Rec: 0.6024 | F1: 0.5579\n",
      "Client 4 - Acc: 0.3107 | Prec: 0.5208 | Rec: 0.5970 | F1: 0.5324\n",
      "Client 5 - Acc: 0.3750 | Prec: 0.5524 | Rec: 0.6208 | F1: 0.5646\n",
      "\n",
      "=== Round 9 ===\n",
      "Client 1 - Acc: 0.3250 | Prec: 0.4940 | Rec: 0.5625 | F1: 0.5070\n",
      "Client 2 - Acc: 0.3893 | Prec: 0.5506 | Rec: 0.6048 | F1: 0.5580\n",
      "Client 3 - Acc: 0.3357 | Prec: 0.5286 | Rec: 0.5851 | F1: 0.5339\n",
      "Client 4 - Acc: 0.2679 | Prec: 0.4750 | Rec: 0.5506 | F1: 0.4850\n",
      "Client 5 - Acc: 0.3393 | Prec: 0.5280 | Rec: 0.6048 | F1: 0.5412\n",
      "\n",
      "=== Round 10 ===\n",
      "Client 1 - Acc: 0.3714 | Prec: 0.5244 | Rec: 0.5726 | F1: 0.5305\n",
      "Client 2 - Acc: 0.3500 | Prec: 0.5360 | Rec: 0.5863 | F1: 0.5382\n",
      "Client 3 - Acc: 0.3500 | Prec: 0.5208 | Rec: 0.5637 | F1: 0.5220\n",
      "Client 4 - Acc: 0.3036 | Prec: 0.5042 | Rec: 0.5792 | F1: 0.5168\n",
      "Client 5 - Acc: 0.3357 | Prec: 0.5155 | Rec: 0.5940 | F1: 0.5317\n",
      "\n",
      "=== Round 11 ===\n",
      "Client 1 - Acc: 0.3429 | Prec: 0.5202 | Rec: 0.5887 | F1: 0.5315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m    121\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m loss = criterion(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m, labels)\n\u001b[32m    123\u001b[39m loss.backward()\n\u001b[32m    124\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\efficientnet_pytorch\\model.py:314\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"EfficientNet's forward function.\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m \u001b[33;03m    Output of this model after processing.\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# Convolution layers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# Pooling and final linear layer\u001b[39;00m\n\u001b[32m    316\u001b[39m x = \u001b[38;5;28mself\u001b[39m._avg_pooling(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\efficientnet_pytorch\\model.py:296\u001b[39m, in \u001b[36mEfficientNet.extract_features\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m drop_connect_rate:\n\u001b[32m    295\u001b[39m         drop_connect_rate *= \u001b[38;5;28mfloat\u001b[39m(idx) / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._blocks)  \u001b[38;5;66;03m# scale drop connect_rate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_connect_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# Head\u001b[39;00m\n\u001b[32m    299\u001b[39m x = \u001b[38;5;28mself\u001b[39m._swish(\u001b[38;5;28mself\u001b[39m._bn1(\u001b[38;5;28mself\u001b[39m._conv_head(x)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\efficientnet_pytorch\\model.py:110\u001b[39m, in \u001b[36mMBConvBlock.forward\u001b[39m\u001b[34m(self, inputs, drop_connect_rate)\u001b[39m\n\u001b[32m    107\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._swish(x)\n\u001b[32m    109\u001b[39m x = \u001b[38;5;28mself\u001b[39m._depthwise_conv(x)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m x = \u001b[38;5;28mself\u001b[39m._swish(x)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Squeeze and Excitation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Improved Mesh-based EfficientNet DFL Code\n",
    "# With Faster Convergence, Cosine Scheduler, Lower Threshold, and Per-Round CSV Saving\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# ======== Dataset Preparation ==========\n",
    "class OcularDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "        return self.transform(img), labels\n",
    "\n",
    "# ======== Model Definition ==============\n",
    "def get_model():\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
    "    model._fc = nn.Linear(model._fc.in_features, 8)\n",
    "    return model\n",
    "\n",
    "# ======== Load & Prepare Data ===========\n",
    "base_path = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/ODIR-5K/ODIR-5K\"\n",
    "train_images_dir = os.path.join(base_path, \"Training Images\")\n",
    "data_file = os.path.join(base_path, \"data.xlsx\")\n",
    "df = pd.read_excel(data_file)\n",
    "label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n",
    "\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    rows.append({\"filename\": r[\"Left-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "    rows.append({\"filename\": r[\"Right-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "\n",
    "df_images = pd.DataFrame(rows)\n",
    "\n",
    "# ======== Data Split ============\n",
    "total_len = len(df_images)\n",
    "split_size = total_len // 5\n",
    "splits = [split_size] * 5\n",
    "splits[-1] += total_len - sum(splits)\n",
    "clients_dfs = random_split(df_images, splits)\n",
    "\n",
    "# ======== Config ================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 5\n",
    "ROUNDS = 30\n",
    "lr = 5e-4\n",
    "\n",
    "# ======== Weighted Aggregation ============\n",
    "def aggregate_weights(weights_list, client_sizes):\n",
    "    total = sum(client_sizes)\n",
    "    avg_weights = []\n",
    "    for weights in zip(*weights_list):\n",
    "        avg = sum(w * s for w, s in zip(weights, client_sizes)) / total\n",
    "        avg_weights.append(avg)\n",
    "    return avg_weights\n",
    "\n",
    "# ======== Mesh Topology ==========\n",
    "neighbors = {i: [j for j in range(5) if j != i] for i in range(5)}\n",
    "\n",
    "# ======== Initialize Clients ============\n",
    "clients = []\n",
    "for idx in range(5):\n",
    "    df = clients_dfs[idx].dataset.iloc[clients_dfs[idx].indices]\n",
    "    dataset = OcularDataset(df, train_images_dir)\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, val_ds = random_split(dataset, [split, len(dataset) - split])\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=16)\n",
    "    clients.append({\n",
    "        \"model\": get_model().to(device),\n",
    "        \"train_loader\": train_loader,\n",
    "        \"val_loader\": val_loader\n",
    "    })\n",
    "\n",
    "# ======== Training Loop ============\n",
    "logs = [pd.DataFrame(columns=['Round', 'Loss', 'Accuracy', 'Precision', 'Recall', 'F1']) for _ in range(5)]\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "log_dir = \"mesh_logs_EfficientNet\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "for rnd in range(1, ROUNDS + 1):\n",
    "    print(f\"\\n=== Round {rnd} ===\")\n",
    "    weights_list, client_sizes = [], []\n",
    "\n",
    "    for idx, client in enumerate(clients):\n",
    "        model = client['model']\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for images, labels in client['train_loader']:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(images), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in client['val_loader']:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                total_loss += criterion(outputs, labels).item()\n",
    "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend((preds > 0.3).astype(int))  # Lower threshold\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "\n",
    "        print(f\"Client {idx+1} - Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "        logs[idx] = pd.concat([\n",
    "            logs[idx],\n",
    "            pd.DataFrame([[rnd, total_loss, acc, prec, rec, f1]], columns=logs[idx].columns)\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        weights_list.append([val.cpu().detach().numpy() for val in model.state_dict().values()])\n",
    "        client_sizes.append(len(client['train_loader'].dataset))\n",
    "\n",
    "    # Aggregation\n",
    "    for i in range(5):\n",
    "        neighbor_indices = neighbors[i] + [i]\n",
    "        selected_weights = [weights_list[j] for j in neighbor_indices]\n",
    "        selected_sizes = [client_sizes[j] for j in neighbor_indices]\n",
    "        new_weights = aggregate_weights(selected_weights, selected_sizes)\n",
    "        state_dict = clients[i]['model'].state_dict()\n",
    "        new_state = {k: torch.tensor(v).to(device) for k, v in zip(state_dict.keys(), new_weights)}\n",
    "        clients[i]['model'].load_state_dict(new_state)\n",
    "\n",
    "    # Save logs after each round\n",
    "    for i in range(5):\n",
    "        logs[i].to_csv(\n",
    "            f\"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/mesh_logs_EfficientNet/client_{i+1}_performance_log.csv\",\n",
    "            index=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4d6b1",
   "metadata": {},
   "source": [
    "EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5f7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "=== Round 1 ===\n",
      "Client 1 - Acc: 0.1464 | Prec: 0.3869 | Rec: 0.6339 | F1: 0.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\427025815.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 - Acc: 0.1607 | Prec: 0.4115 | Rec: 0.6458 | F1: 0.4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\427025815.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 3 - Acc: 0.1286 | Prec: 0.3740 | Rec: 0.6274 | F1: 0.4478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\427025815.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 4 - Acc: 0.1500 | Prec: 0.3976 | Rec: 0.6655 | F1: 0.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\427025815.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 5 - Acc: 0.1679 | Prec: 0.4265 | Rec: 0.6524 | F1: 0.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\427025815.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[idx] = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 2 ===\n",
      "Client 1 - Acc: 0.1679 | Prec: 0.3991 | Rec: 0.6429 | F1: 0.4677\n",
      "Client 2 - Acc: 0.2107 | Prec: 0.4554 | Rec: 0.6667 | F1: 0.5185\n",
      "Client 3 - Acc: 0.1393 | Prec: 0.3928 | Rec: 0.6476 | F1: 0.4676\n",
      "Client 4 - Acc: 0.1714 | Prec: 0.3929 | Rec: 0.6149 | F1: 0.4580\n",
      "Client 5 - Acc: 0.2321 | Prec: 0.4732 | Rec: 0.6792 | F1: 0.5292\n",
      "\n",
      "=== Round 3 ===\n",
      "Client 1 - Acc: 0.2286 | Prec: 0.4562 | Rec: 0.6661 | F1: 0.5170\n",
      "Client 2 - Acc: 0.1786 | Prec: 0.4377 | Rec: 0.6667 | F1: 0.5032\n",
      "Client 3 - Acc: 0.2107 | Prec: 0.4280 | Rec: 0.6161 | F1: 0.4839\n",
      "Client 4 - Acc: 0.1643 | Prec: 0.4027 | Rec: 0.6131 | F1: 0.4633\n",
      "Client 5 - Acc: 0.2357 | Prec: 0.4795 | Rec: 0.6845 | F1: 0.5362\n",
      "\n",
      "=== Round 4 ===\n",
      "Client 1 - Acc: 0.2036 | Prec: 0.4277 | Rec: 0.6036 | F1: 0.4770\n",
      "Client 2 - Acc: 0.2000 | Prec: 0.4539 | Rec: 0.6500 | F1: 0.5067\n",
      "Client 3 - Acc: 0.2071 | Prec: 0.4255 | Rec: 0.6131 | F1: 0.4808\n",
      "Client 4 - Acc: 0.2071 | Prec: 0.4235 | Rec: 0.6226 | F1: 0.4811\n",
      "Client 5 - Acc: 0.2393 | Prec: 0.4530 | Rec: 0.6095 | F1: 0.4958\n",
      "\n",
      "=== Round 5 ===\n",
      "Client 1 - Acc: 0.2357 | Prec: 0.4363 | Rec: 0.6018 | F1: 0.4836\n",
      "Client 2 - Acc: 0.2714 | Prec: 0.4824 | Rec: 0.6304 | F1: 0.5188\n",
      "Client 3 - Acc: 0.2286 | Prec: 0.4548 | Rec: 0.6286 | F1: 0.5030\n",
      "Client 4 - Acc: 0.2036 | Prec: 0.4211 | Rec: 0.6042 | F1: 0.4735\n",
      "Client 5 - Acc: 0.2464 | Prec: 0.4693 | Rec: 0.6435 | F1: 0.5177\n",
      "\n",
      "=== Round 6 ===\n",
      "Client 1 - Acc: 0.2500 | Prec: 0.4387 | Rec: 0.6054 | F1: 0.4863\n",
      "Client 2 - Acc: 0.2643 | Prec: 0.4799 | Rec: 0.6315 | F1: 0.5211\n",
      "Client 3 - Acc: 0.2214 | Prec: 0.4221 | Rec: 0.5702 | F1: 0.4654\n",
      "Client 4 - Acc: 0.2571 | Prec: 0.4474 | Rec: 0.6173 | F1: 0.4972\n",
      "Client 5 - Acc: 0.2786 | Prec: 0.4554 | Rec: 0.5839 | F1: 0.4901\n",
      "\n",
      "=== Round 7 ===\n",
      "Client 1 - Acc: 0.2429 | Prec: 0.4265 | Rec: 0.5661 | F1: 0.4645\n",
      "Client 2 - Acc: 0.2714 | Prec: 0.4836 | Rec: 0.6494 | F1: 0.5306\n",
      "Client 3 - Acc: 0.2321 | Prec: 0.4268 | Rec: 0.5601 | F1: 0.4648\n",
      "Client 4 - Acc: 0.2464 | Prec: 0.4491 | Rec: 0.5798 | F1: 0.4844\n",
      "Client 5 - Acc: 0.2964 | Prec: 0.5000 | Rec: 0.6071 | F1: 0.5225\n",
      "\n",
      "=== Round 8 ===\n",
      "Client 1 - Acc: 0.2750 | Prec: 0.4571 | Rec: 0.5714 | F1: 0.4880\n",
      "Client 2 - Acc: 0.2714 | Prec: 0.4628 | Rec: 0.5887 | F1: 0.4944\n",
      "Client 3 - Acc: 0.2679 | Prec: 0.4827 | Rec: 0.6185 | F1: 0.5196\n",
      "Client 4 - Acc: 0.2179 | Prec: 0.4256 | Rec: 0.5696 | F1: 0.4650\n",
      "Client 5 - Acc: 0.2714 | Prec: 0.4729 | Rec: 0.6113 | F1: 0.5106\n",
      "\n",
      "=== Round 9 ===\n",
      "Client 1 - Acc: 0.2607 | Prec: 0.4396 | Rec: 0.5554 | F1: 0.4719\n",
      "Client 2 - Acc: 0.2929 | Prec: 0.4750 | Rec: 0.5851 | F1: 0.5024\n",
      "Client 3 - Acc: 0.2786 | Prec: 0.4890 | Rec: 0.6018 | F1: 0.5161\n",
      "Client 4 - Acc: 0.2643 | Prec: 0.4399 | Rec: 0.5821 | F1: 0.4824\n",
      "Client 5 - Acc: 0.2679 | Prec: 0.4670 | Rec: 0.5946 | F1: 0.5008\n",
      "\n",
      "=== Round 10 ===\n",
      "Client 1 - Acc: 0.2536 | Prec: 0.4369 | Rec: 0.5429 | F1: 0.4637\n",
      "Client 2 - Acc: 0.3036 | Prec: 0.4946 | Rec: 0.5750 | F1: 0.5098\n",
      "Client 3 - Acc: 0.2571 | Prec: 0.4325 | Rec: 0.5637 | F1: 0.4713\n",
      "Client 4 - Acc: 0.2571 | Prec: 0.4417 | Rec: 0.5661 | F1: 0.4769\n",
      "Client 5 - Acc: 0.2964 | Prec: 0.4795 | Rec: 0.5661 | F1: 0.4995\n",
      "\n",
      "=== Round 11 ===\n",
      "Client 1 - Acc: 0.2750 | Prec: 0.4390 | Rec: 0.5250 | F1: 0.4598\n",
      "Client 2 - Acc: 0.2643 | Prec: 0.4595 | Rec: 0.5637 | F1: 0.4835\n",
      "Client 3 - Acc: 0.3179 | Prec: 0.4911 | Rec: 0.5667 | F1: 0.5073\n",
      "Client 4 - Acc: 0.2393 | Prec: 0.4095 | Rec: 0.5423 | F1: 0.4491\n",
      "Client 5 - Acc: 0.3143 | Prec: 0.4942 | Rec: 0.5786 | F1: 0.5117\n",
      "\n",
      "=== Round 12 ===\n",
      "Client 1 - Acc: 0.3429 | Prec: 0.4854 | Rec: 0.5482 | F1: 0.4974\n",
      "Client 2 - Acc: 0.2857 | Prec: 0.4958 | Rec: 0.5881 | F1: 0.5138\n",
      "Client 3 - Acc: 0.2821 | Prec: 0.4652 | Rec: 0.5821 | F1: 0.4994\n",
      "Client 4 - Acc: 0.2571 | Prec: 0.4244 | Rec: 0.5310 | F1: 0.4519\n",
      "Client 5 - Acc: 0.2964 | Prec: 0.4785 | Rec: 0.5792 | F1: 0.4992\n",
      "\n",
      "=== Round 13 ===\n",
      "Client 1 - Acc: 0.3000 | Prec: 0.4735 | Rec: 0.5625 | F1: 0.4956\n",
      "Client 2 - Acc: 0.3036 | Prec: 0.4833 | Rec: 0.5750 | F1: 0.5030\n",
      "Client 3 - Acc: 0.3000 | Prec: 0.4894 | Rec: 0.5839 | F1: 0.5099\n",
      "Client 4 - Acc: 0.3214 | Prec: 0.4741 | Rec: 0.5435 | F1: 0.4884\n",
      "Client 5 - Acc: 0.2964 | Prec: 0.4815 | Rec: 0.5714 | F1: 0.5007\n",
      "\n",
      "=== Round 14 ===\n",
      "Client 1 - Acc: 0.2893 | Prec: 0.4708 | Rec: 0.5732 | F1: 0.4967\n",
      "Client 2 - Acc: 0.3464 | Prec: 0.5054 | Rec: 0.5690 | F1: 0.5167\n",
      "Client 3 - Acc: 0.3036 | Prec: 0.4821 | Rec: 0.5625 | F1: 0.4996\n",
      "Client 4 - Acc: 0.2607 | Prec: 0.4235 | Rec: 0.5167 | F1: 0.4459\n",
      "Client 5 - Acc: 0.3357 | Prec: 0.5167 | Rec: 0.5845 | F1: 0.5283\n",
      "\n",
      "=== Round 15 ===\n",
      "Client 1 - Acc: 0.3321 | Prec: 0.4813 | Rec: 0.5643 | F1: 0.5019\n",
      "Client 2 - Acc: 0.2929 | Prec: 0.4851 | Rec: 0.5673 | F1: 0.4979\n",
      "Client 3 - Acc: 0.2929 | Prec: 0.4723 | Rec: 0.5738 | F1: 0.5006\n",
      "Client 4 - Acc: 0.3071 | Prec: 0.4732 | Rec: 0.5643 | F1: 0.4973\n",
      "Client 5 - Acc: 0.3321 | Prec: 0.4929 | Rec: 0.5702 | F1: 0.5098\n",
      "\n",
      "=== Round 16 ===\n",
      "Client 1 - Acc: 0.2964 | Prec: 0.4774 | Rec: 0.5661 | F1: 0.4976\n",
      "Client 2 - Acc: 0.2750 | Prec: 0.4935 | Rec: 0.5839 | F1: 0.5082\n",
      "Client 3 - Acc: 0.2929 | Prec: 0.4693 | Rec: 0.5577 | F1: 0.4905\n",
      "Client 4 - Acc: 0.3036 | Prec: 0.4590 | Rec: 0.5548 | F1: 0.4850\n",
      "Client 5 - Acc: 0.3429 | Prec: 0.5226 | Rec: 0.5857 | F1: 0.5321\n",
      "\n",
      "=== Round 17 ===\n",
      "Client 1 - Acc: 0.3107 | Prec: 0.4718 | Rec: 0.5500 | F1: 0.4890\n",
      "Client 2 - Acc: 0.3250 | Prec: 0.4869 | Rec: 0.5351 | F1: 0.4912\n",
      "Client 3 - Acc: 0.2857 | Prec: 0.4833 | Rec: 0.5940 | F1: 0.5118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    123\u001b[39m scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=\u001b[32m5\u001b[39m, gamma=\u001b[32m0.5\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_loader\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mOcularDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     37\u001b[39m img = Image.open(img_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m labels = torch.tensor(row[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m, labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\PIL\\Image.py:2365\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2353\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2354\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2355\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2356\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2357\u001b[39m         )\n\u001b[32m   2358\u001b[39m         box = (\n\u001b[32m   2359\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2360\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2361\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2362\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2363\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Improved Mesh-based EfficientNet DFL Code\n",
    "# With Weighted Aggregation, Augmentations, Scheduler, Better Loss, and Per-Round CSV Saving\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# ======== Dataset Preparation ==========\n",
    "class OcularDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            T.RandomRotation(10),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "        return self.transform(img), labels\n",
    "\n",
    "# ======== Model Definition ==============\n",
    "def get_model():\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "    model._fc = nn.Linear(model._fc.in_features, 8)\n",
    "    return model\n",
    "\n",
    "# ======== Load & Prepare Data ===========\n",
    "base_path = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/ODIR-5K/ODIR-5K\"\n",
    "train_images_dir = os.path.join(base_path, \"Training Images\")\n",
    "data_file = os.path.join(base_path, \"data.xlsx\")\n",
    "df = pd.read_excel(data_file)\n",
    "label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n",
    "\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    rows.append({\"filename\": r[\"Left-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "    rows.append({\"filename\": r[\"Right-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "\n",
    "df_images = pd.DataFrame(rows)\n",
    "\n",
    "# Calculate pos_weight for BCEWithLogitsLoss\n",
    "label_matrix = np.array(df_images[\"labels\"].to_list())\n",
    "counts = label_matrix.sum(axis=0)\n",
    "total = len(df_images)\n",
    "pos_weight = torch.tensor((total - counts) / counts, dtype=torch.float32)\n",
    "\n",
    "# ======== Data Split ============\n",
    "total_len = len(df_images)\n",
    "split_size = total_len // 5\n",
    "splits = [split_size] * 5\n",
    "splits[-1] += total_len - sum(splits)\n",
    "clients_dfs = random_split(df_images, splits)\n",
    "\n",
    "# ======== Config ================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 10\n",
    "ROUNDS = 50\n",
    "lr = 1e-4\n",
    "\n",
    "# ======== Weighted Aggregation ============\n",
    "def aggregate_weights(weights_list, client_sizes):\n",
    "    total = sum(client_sizes)\n",
    "    avg_weights = []\n",
    "    for weights in zip(*weights_list):\n",
    "        avg = sum(w * s for w, s in zip(weights, client_sizes)) / total\n",
    "        avg_weights.append(avg)\n",
    "    return avg_weights\n",
    "\n",
    "# ======== Mesh Topology ==========\n",
    "neighbors = {i: [j for j in range(5) if j != i] for i in range(5)}\n",
    "\n",
    "# ======== Initialize Clients ============\n",
    "clients = []\n",
    "for idx in range(5):\n",
    "    df = clients_dfs[idx].dataset.iloc[clients_dfs[idx].indices]\n",
    "    dataset = OcularDataset(df, train_images_dir)\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, val_ds = random_split(dataset, [split, len(dataset) - split])\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=16)\n",
    "    clients.append({\n",
    "        \"model\": get_model().to(device),\n",
    "        \"train_loader\": train_loader,\n",
    "        \"val_loader\": val_loader\n",
    "    })\n",
    "\n",
    "# ======== Training Loop ============\n",
    "logs = [pd.DataFrame(columns=['Round', 'Loss', 'Accuracy', 'Precision', 'Recall', 'F1']) for _ in range(5)]\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "# Make sure output directory exists\n",
    "log_dir = \"mesh_logs_EfficientNet\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "for rnd in range(1, ROUNDS + 1):\n",
    "    print(f\"\\n=== Round {rnd} ===\")\n",
    "    weights_list, client_sizes = [], []\n",
    "\n",
    "    for idx, client in enumerate(clients):\n",
    "        model = client['model']\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for images, labels in client['train_loader']:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(images), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in client['val_loader']:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                total_loss += criterion(outputs, labels).item()\n",
    "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend((preds > 0.5).astype(int))\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "\n",
    "        print(f\"Client {idx+1} - Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "        logs[idx] = pd.concat([\n",
    "            logs[idx],\n",
    "            pd.DataFrame([[rnd, total_loss, acc, prec, rec, f1]], columns=logs[idx].columns)\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        weights_list.append([val.cpu().detach().numpy() for val in model.state_dict().values()])\n",
    "        client_sizes.append(len(client['train_loader'].dataset))\n",
    "\n",
    "    # Aggregation\n",
    "    for i in range(5):\n",
    "        neighbor_indices = neighbors[i] + [i]\n",
    "        selected_weights = [weights_list[j] for j in neighbor_indices]\n",
    "        selected_sizes = [client_sizes[j] for j in neighbor_indices]\n",
    "        new_weights = aggregate_weights(selected_weights, selected_sizes)\n",
    "        state_dict = clients[i]['model'].state_dict()\n",
    "        new_state = {k: torch.tensor(v).to(device) for k, v in zip(state_dict.keys(), new_weights)}\n",
    "        clients[i]['model'].load_state_dict(new_state)\n",
    "\n",
    "    # ======== Save Logs After Each Round ==========\n",
    "    for i in range(5):\n",
    "        logs[i].to_csv(\n",
    "        f\"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/mesh_logs_EfficientNet/client_{i+1}_performance_log.csv\",\n",
    "        index=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2cb97",
   "metadata": {},
   "source": [
    "Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90f825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Round 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\655558932.py:133: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Loss: 20.4446, Acc: 0.0179, Prec: 0.3485, Rec: 0.8833, F1: 0.4865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\655558932.py:133: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 - Loss: 21.1234, Acc: 0.0179, Prec: 0.3415, Rec: 0.8804, F1: 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\655558932.py:133: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 3 - Loss: 19.8812, Acc: 0.0107, Prec: 0.3382, Rec: 0.9018, F1: 0.4816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\655558932.py:133: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 4 - Loss: 20.9530, Acc: 0.0179, Prec: 0.3474, Rec: 0.8732, F1: 0.4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitoo\\AppData\\Local\\Temp\\ipykernel_21552\\655558932.py:133: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 5 - Loss: 20.9755, Acc: 0.0000, Prec: 0.3283, Rec: 0.8792, F1: 0.4684\n",
      "\n",
      "=== Round 2 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m client[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].train()\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_loader\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mOcularDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     32\u001b[39m row = \u001b[38;5;28mself\u001b[39m.df.iloc[idx]\n\u001b[32m     33\u001b[39m img_path = os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, row[\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m labels = torch.tensor(row[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m], dtype=torch.float32)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform(img), labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\PIL\\Image.py:993\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    991\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    997\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aitoo\\anaconda3\\envs\\fl_env\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Swin Transformer with DFL-based Mesh Topology (FedAvg) ===\n",
    "# ==============================================================\n",
    "# Uses simple average instead of weighted aggregation\n",
    "# ==============================================================\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import ast\n",
    "import timm\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "class OcularDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "        return self.transform(img), labels\n",
    "\n",
    "# === Model ===\n",
    "def get_model():\n",
    "    model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True, num_classes=8)\n",
    "    return model\n",
    "\n",
    "# === Settings ===\n",
    "base_path = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/ODIR-5K/ODIR-5K\"\n",
    "data_file = os.path.join(base_path, \"data.xlsx\")\n",
    "image_dir = os.path.join(base_path, \"Training Images\")\n",
    "\n",
    "label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n",
    "df = pd.read_excel(data_file)\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    rows.append({\"filename\": r[\"Left-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "    rows.append({\"filename\": r[\"Right-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "df_images = pd.DataFrame(rows)\n",
    "df_images[\"labels\"] = df_images[\"labels\"].apply(lambda x: ast.literal_eval(str(x)))\n",
    "\n",
    "total_len = len(df_images)\n",
    "split_size = total_len // 5\n",
    "splits = [split_size] * 5\n",
    "splits[-1] += total_len - sum(splits)\n",
    "clients_dfs = random_split(df_images, splits)\n",
    "\n",
    "# === Mesh Topology Neighbors ===\n",
    "neighbors = {\n",
    "    0: [1, 2, 3, 4],\n",
    "    1: [0, 2, 3, 4],\n",
    "    2: [0, 1, 3, 4],\n",
    "    3: [0, 1, 2, 4],\n",
    "    4: [0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Logs ===\n",
    "os.makedirs(\"mesh_logs_swin_fedavg\", exist_ok=True)\n",
    "logs = [pd.DataFrame(columns=[\"Round\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        for _ in range(5)]\n",
    "\n",
    "# === Training ===\n",
    "clients = []\n",
    "for idx in range(5):\n",
    "    dataset = OcularDataset(clients_dfs[idx].dataset.iloc[clients_dfs[idx].indices], image_dir)\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, val_ds = random_split(dataset, [split, len(dataset) - split])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=8)\n",
    "    model = get_model().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    pos_weight = torch.tensor([3.0] * 8).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    clients.append({\n",
    "        'model': model, 'optimizer': optimizer, 'train_loader': train_loader,\n",
    "        'val_loader': val_loader, 'criterion': criterion\n",
    "    })\n",
    "\n",
    "# === Mesh Rounds ===\n",
    "for round_num in range(1, 31):\n",
    "    print(f\"\\n=== Round {round_num} ===\")\n",
    "    weights = []\n",
    "\n",
    "    # Local Training\n",
    "    for i, client in enumerate(clients):\n",
    "        client['model'].train()\n",
    "        for epoch in range(3):\n",
    "            for x, y in client['train_loader']:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                client['optimizer'].zero_grad()\n",
    "                pred = client['model'](x)\n",
    "                loss = client['criterion'](pred, y)\n",
    "                loss.backward()\n",
    "                client['optimizer'].step()\n",
    "\n",
    "    # Evaluation\n",
    "    for i, client in enumerate(clients):\n",
    "        client['model'].eval()\n",
    "        y_true, y_pred = [], []\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in client['val_loader']:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = client['model'](x)\n",
    "                val_loss += client['criterion'](out, y).item()\n",
    "                preds = torch.sigmoid(out).cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend((preds > 0.3).astype(int))\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "\n",
    "        logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n",
    "                                                   columns=logs[i].columns)], ignore_index=True)\n",
    "        weights.append(copy.deepcopy(client['model'].state_dict()))\n",
    "\n",
    "        print(f\"Client {i+1} - Loss: {val_loss:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # FedAvg Aggregation\n",
    "    for i in range(5):\n",
    "        neighbor_weights = [weights[j] for j in neighbors[i]] + [weights[i]]\n",
    "        avg_weights = copy.deepcopy(neighbor_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            for j in range(1, len(neighbor_weights)):\n",
    "                avg_weights[key] += neighbor_weights[j][key]\n",
    "            avg_weights[key] /= len(neighbor_weights)\n",
    "        clients[i]['model'].load_state_dict(avg_weights)\n",
    "\n",
    "# === Save Logs ===\n",
    "for i, log in enumerate(logs):\n",
    "    log.to_csv(f\"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/mesh_logs_EfficientNet/client_{i+1}_performance_log.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Mesh-based DFL with Swin Transformer (FedAvg) Completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21df5e",
   "metadata": {},
   "source": [
    "mobilenet_v3_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97753c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_v3_large - Mesh-based Decentralized Federated Learning (DFL)\n",
    "# ===========================================================\n",
    "# Uses FedAvg for model aggregation\n",
    "# ===========================================================\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T, models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import ast\n",
    "\n",
    "# === Dataset & Loader ===\n",
    "class OcularDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.image_dir, row['filename'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n",
    "        return self.transform(img), labels\n",
    "\n",
    "# === Model ===\n",
    "def get_model():\n",
    "    model = models.mobilenet_v3_large(pretrained=True)\n",
    "    model.classifier[3] = nn.Linear(model.classifier[3].in_features, 8)\n",
    "    return model\n",
    "\n",
    "# === FedAvg Aggregation ===\n",
    "def fedavg(state_dicts):\n",
    "    avg_state = copy.deepcopy(state_dicts[0])\n",
    "    for key in avg_state.keys():\n",
    "        for i in range(1, len(state_dicts)):\n",
    "            avg_state[key] += state_dicts[i][key]\n",
    "        avg_state[key] /= len(state_dicts)\n",
    "    return avg_state\n",
    "\n",
    "# === Settings ===\n",
    "base_path = \"C:/Users/aitoo/OneDrive/Desktop/FYP-Code/ODIR-5K/ODIR-5K\"\n",
    "data_file = os.path.join(base_path, \"data.xlsx\")\n",
    "image_dir = os.path.join(base_path, \"Training Images\")\n",
    "\n",
    "label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n",
    "df = pd.read_excel(data_file)\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    rows.append({\"filename\": r[\"Left-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "    rows.append({\"filename\": r[\"Right-Fundus\"], \"labels\": r[label_cols].values.astype(int).tolist()})\n",
    "df_images = pd.DataFrame(rows)\n",
    "df_images[\"labels\"] = df_images[\"labels\"].apply(lambda x: ast.literal_eval(str(x)))\n",
    "\n",
    "total_len = len(df_images)\n",
    "split_size = total_len // 5\n",
    "splits = [split_size] * 5\n",
    "splits[-1] += total_len - sum(splits)\n",
    "clients_dfs = random_split(df_images, splits)\n",
    "\n",
    "# === Mesh Topology Neighbors ===\n",
    "neighbors = {\n",
    "    0: [1, 2, 3, 4],\n",
    "    1: [0, 2, 3, 4],\n",
    "    2: [0, 1, 3, 4],\n",
    "    3: [0, 1, 2, 4],\n",
    "    4: [0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "# === Device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Logs ===\n",
    "os.makedirs(\"mesh_logs_mobilenetv3\", exist_ok=True)\n",
    "logs = [pd.DataFrame(columns=[\"Round\", \"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "        for _ in range(5)]\n",
    "\n",
    "# === Training ===\n",
    "clients = []\n",
    "for idx in range(5):\n",
    "    dataset = OcularDataset(clients_dfs[idx].dataset.iloc[clients_dfs[idx].indices], image_dir)\n",
    "    split = int(0.8 * len(dataset))\n",
    "    train_ds, val_ds = random_split(dataset, [split, len(dataset) - split])\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=8)\n",
    "    model = get_model().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    pos_weight = torch.tensor([3.0] * 8).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    clients.append({\n",
    "        'model': model, 'optimizer': optimizer, 'train_loader': train_loader,\n",
    "        'val_loader': val_loader, 'criterion': criterion\n",
    "    })\n",
    "\n",
    "# === Mesh Rounds ===\n",
    "for round_num in range(1, 31):\n",
    "    print(f\"\\n=== Round {round_num} ===\")\n",
    "    weights = []\n",
    "\n",
    "    # Local Training\n",
    "    for i, client in enumerate(clients):\n",
    "        client['model'].train()\n",
    "        for epoch in range(3):\n",
    "            for x, y in client['train_loader']:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                client['optimizer'].zero_grad()\n",
    "                pred = client['model'](x)\n",
    "                loss = client['criterion'](pred, y)\n",
    "                loss.backward()\n",
    "                client['optimizer'].step()\n",
    "\n",
    "    # Evaluation\n",
    "    for i, client in enumerate(clients):\n",
    "        client['model'].eval()\n",
    "        y_true, y_pred = [], []\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in client['val_loader']:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = client['model'](x)\n",
    "                val_loss += client['criterion'](out, y).item()\n",
    "                preds = torch.sigmoid(out).cpu().numpy()\n",
    "                y_true.extend(y.cpu().numpy())\n",
    "                y_pred.extend((preds > 0.3).astype(int))\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"samples\", zero_division=0)\n",
    "\n",
    "        logs[i] = pd.concat([logs[i], pd.DataFrame([[round_num, val_loss, acc, prec, rec, f1]],\n",
    "                                                   columns=logs[i].columns)], ignore_index=True)\n",
    "        weights.append(copy.deepcopy(client['model'].state_dict()))\n",
    "\n",
    "        print(f\"Client {i+1} - Loss: {val_loss:.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Aggregation using FedAvg\n",
    "    for i in range(5):\n",
    "        neighbor_weights = [weights[j] for j in neighbors[i]] + [weights[i]]\n",
    "        agg_weights = fedavg(neighbor_weights)\n",
    "        clients[i]['model'].load_state_dict(agg_weights)\n",
    "\n",
    "# === Save Logs ===\n",
    "for i, log in enumerate(logs):\n",
    "    log.to_csv(f\"mesh_logs_mobilenetv3/client_{i+1}_performance_log.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Mesh-based DFL with MobileNetV3 (FedAvg) Completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
